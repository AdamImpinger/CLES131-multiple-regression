---
title: "Exploring drought effects with regression"
format: pdf
---

# Synthesis from the International Drought experiment

We are going to replicate some of the regressions and figures from Smith et al. 2024. This PNAS paper was published with a link to Dryad, a popular data repository in ecology community run on FAIR principles. FAIR stands for Findable, Accessible, Interoperable, and Reusable. 

### Q1
The full contents of the Dryad repository are in `data/`. From either the README or .R file, can  you tell what types of information are in each .csv? How would you assess the repository in light of the FAIR criteria?
findable acessable interoperable resuable

I would say yes. Hard to assess findable because we did not find the data ourselves. 
However it does seem to be accessable considering we are accessing it.
It is interoperable, there is a program that is used to create scripts etc.
Reusable, also yes. 


## Wrangling data

Since the goal of this exercise is to use linear regression and practice plotting, I have provided the code below for wrangling the data into a single dataframe:

```{r}
#| echo: false
library(tidyverse)

# Figure 3 regression predictors
map <- read_csv("data/datappt_MAP2.csv") |> 
  select(-1, -X)
prior_ppt <- read_csv("data/datappt_pryr.csv") |> 
  select(-1)
map_cv <- read_csv("data/Dryad_submission_datappt_cv2.csv") |> 
  select(-1)
ai <- read_csv("data/datappt_ai3.csv") |> 
  select(-1)
sand <- read_csv("data/Dryad_submission_datapptsand2.csv") |> 
  select(-1)
graminoid <- read_csv("data/Dryad_submission_gram.ave.csv") |> 
  select(-1)
richness <- read_csv("data/Dryad_submission_datappt_rich2.csv") |> 
  select(-1)

# Figure 4 drought severity
drought_severity <- read_csv("data/data.Drtsite.csv") |> 
  select(-1)

# Create full dataset from above
data_all <- read_csv("data/data.hab.csv") |> 
  select(-1) |> 
  rename(habitat_type = habitat.type) |> 
  left_join(map) |> 
  relocate(precip, .after = Sig) |> 
  left_join(prior_ppt |> 
              select(site_code, drtpct_map730)) |> 
  left_join(map_cv |> 
              select(site_code, cv_ppt_inter)) |> 
  left_join(ai |> 
              select(site_code, logAI)) |> 
  left_join(sand|> 
              select(site_code, sand_mean)) |> 
  left_join(graminoid |> 
              select(site_code, prop.ave)) |> 
  left_join(richness |> 
              select(site_code, average.richness)) |> 
  left_join(drought_severity|> 
              select(site_code, mean_drt2)) |> 
  rename(DR = mean_DS3, # Drought response
         MAP = precip,
         prev_precip = drtpct_map730,
         MAP_cv = cv_ppt_inter,
         lnAridity = logAI,
         gram_prop = prop.ave,
         rich_mean = average.richness,
         DS = mean_drt2) # Drought severity
```

## Univariate drivers of drought response (Fig. 3)

### Q2
Recall previous notes on running simple linear regressions using `lm()`. Replicate all of the simple linear regressions in Fig. 3. 
 
```{r}
lm(data_all$MAP~data_all$DR)
lm(data_all$prev_precip~data_all$DR)
lm(data_all$MAP_cv~data_all$DR)
lm(data_all$lnAridity~data_all$DR)
lm(data_all$sand_mean~data_all$DR)
lm(data_all$gram_prop~data_all$DR)
lm(data_all$rich_mean~data_all$DR)
```

Do your outputs match those of the published figure?

### Q3 (optional)
From the published R code, it appears that Fig. 3 was made in separate pieces. Recreate the figure to the best of your ability with a single code chunk, using `facet_wrap()`. 
 
```{r}
 
a <- data_all |> 
  pivot_longer(cols = MAP:DS,
                  names_to = "xaxes", values_to = "value")

p <- ggplot(data = a, mapping = aes(x = value, y = DR)) + 
  geom_point() +
  geom_smooth(method = "lm")+

facet_wrap( ~ xaxes, scales = "free")
p
```

## Testing the Inverse Texture hypothesis
See Prof. Guo's slides on the inverse texture hypothesis. Under this hypothesis, there should be a significant interaction term between which two variables?


### Q4
Conduct a multiple regression to test the inverse texture hypothesis. Is there statistical support?

```{r}

```

## Importance of drought severity (Fig. 4)

### Q5
Replicate the simple linear regression in Fig. 4. 

```{r}

```

How much variation in drought response is explained by drought severity? How does this compare to the coefficient of determination (R^2) of the predictors in Fig. 3?

### Q6
It can be unsatisfying to run numerous simple linear regressions. We also increase the risk of Type I error, meaning that due to the large number of hypothesis tests, we are more likely to see a false positive even if that relationship isn't truly significant. 

In the supplement Table S8, Smith et al. 2024 show the output of a multiple regression. Replicate this below. 

```{r}

```

How much more variation in drought response is predicted by this ensemble of predictor variables? 

Which variable does the heavy lifting, and how does this compare to the results of the simple linear regression?


### Bonus 1
One common consideration in multiple regression is multicollinearity of the predictor variable. Look up the documentation for `pairs()` and use it on all of the predictor columns in `data_all`. Which predictors are strongly correlated? What does this suggest about their usage in a multiple regression model?

```{r}

```


### Bonus 2
When there are multiple possible predictor variables to choose from, we can use model selection methods to identify the 'best' overall model. 'Best' cannot be only based on high R^2 values, as that would promote overfitting such that conclusions are unlikely to be applicable to a new context. Therefore, 'best' is often determined by the tradeoff between high model fit and low model complexity. Often, the target statistic to minimize is an information criterion, which consists of two components:

$IC = likelihood - penalty$ 

where the likelihood is a metric of model fit and the penalty is a metric of model complexity. The adjusted $R^2$ in the `lm()` output is one such IC. Another common information criterion you may encounter is the AIC, or Akaike Information Criterion:

$AIC = -2ln(L) + 2k$

where ln(L) is the log-likelihood and k is the number of parameters in the model. A model with a smaller AIC value is deemed 'better' at predicting new data than a model with a larger AIC value, given the same response variable being modeled. 

You may use `AIC()` on your model object to determine the AIC value. Try a backward selection approach (using all predictors available) and sequentially reduce number of predictors (removing the highest p-value) until the model's AIC stops decreasing. How does this model compare to the multiple regression model used by Smith et al. (2024)?

```{r}

```


